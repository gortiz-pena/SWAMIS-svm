{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def libsvm_read(f):\n",
    "    '''\n",
    "    This just grabs Derek's svm_traintest.txt files\n",
    "    and reads the data to that I can play with it easily. \n",
    "    This was more difficult than I expected, \n",
    "    and perhaps can be streamlined by writing better output files\n",
    "    when extracting the parameters from the SWAMIS data.\n",
    "    '''\n",
    "    data = np.genfromtxt(f, dtype=str)\n",
    "    # y are the labels, and I've set them to 1 for positive and -1 for negative instances\n",
    "    # This is the convention in the SVM literature and lets me perform calculations on scikit-learn outputs\n",
    "    y = data[:, 0].astype(int)\n",
    "    y = (y - 5)/5\n",
    "\n",
    "    NN = data[:, 1:].shape\n",
    "    n = NN[0]\n",
    "    m = NN[1]\n",
    "\n",
    "    x = np.zeros(NN, dtype='<U22')\n",
    "    for i in range(m):\n",
    "        x[:, i] = np.char.replace(data[:, i+1], '{}:'.format(i+1), '')\n",
    "\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xi_alpha_est(C, x, y):\n",
    "    '''\n",
    "    The Xi-alpha estimators for the error rate, recall, precision, and F1 \n",
    "    of the trained C-support vector classifier C are calculated\n",
    "\n",
    "    input:\n",
    "    C - C-svc object, trained on data x, y\n",
    "    x - data array, contains all objects and their features\n",
    "    y - label array, contains labels for all objects\n",
    "\n",
    "    see Joachims, T.; 2000a\n",
    "    https://www.cs.cornell.edu/people/tj/publications/joachims_00a.pdf\n",
    "    '''\n",
    "    dec = C.decision_function(x)\n",
    "    eps = np.maximum(1 - y*dec, 0)\n",
    "\n",
    "    '''\n",
    "    Important point: alpha = 0 for all of the instances \n",
    "    that aren't support vectors (this isn't clear from the literature and documentation)\n",
    "    Since scikit-learn doesn't store 0 alphas I had to correctly\n",
    "    identify which instances are support vectors and then extract \n",
    "    their alpha values\n",
    "    '''\n",
    "    alpha = np.zeros_like(eps)\n",
    "    ii_sv = np.where(x.all(axis=0) == C.support_vectors_.all(axis=0))\n",
    "    alpha[ii_sv] = np.abs(C.dual_coef_[0])\n",
    "    \n",
    "    n = y.size\n",
    "    inequa = 2*alpha + eps\n",
    "    inequa.shape = n\n",
    "    \n",
    "    d = np.where((inequa >= 1))[0].size\n",
    "    d_plus = np.where(np.logical_and(inequa >= 1, y == 1))[0].size\n",
    "    d_min = np.where(np.logical_and(inequa <= -1, y == -1))[0].size\n",
    "    n_plus = np.where((y == 1))[0].size\n",
    "\n",
    "    Err = d/n\n",
    "    Rec = 1 - d_plus/n_plus\n",
    "    Prec = (n_plus - d_plus)/(n_plus - d_plus + d_min)\n",
    "    F1 = 2*(n_plus - d_plus)/(2*n_plus - d_plus + d_min)\n",
    "\n",
    "    return Err, Rec, Prec, F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your data set.\n",
    "This *could* be made fancy, but I'm still working on implementing this in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = libsvm_read('svm_traintest_with_preB.txt')\n",
    "#Y *= -1 #I'm testing the inverse problem to see how much the performance increases\n",
    "mets = np.zeros((10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I split the data once, to perform the coarse grid search and save those parameters. I will perform a finer search in the vicinity of these parameters to further optimize the problem. This is identical to the process going on inside the loop, except without the testing phase. The initial search parameters are the default that *libsvm* uses for their *easy.py* script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'gamma': 0.026278012976678578, 'C': 1.0905077326652577}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 123456789)\n",
    "\n",
    "scale = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scale.transform(x_train)\n",
    "x_test = scale.transform(x_test)\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': np.logspace(-15, 3, num = 25, base=2), 'C': np.logspace(-2, 15, num = 25, base=2)}]\n",
    "\n",
    "\n",
    "CV = GridSearchCV(SVC(C=1), tuned_parameters, cv=10, scoring='accuracy')\n",
    "CV.fit(x_train, y_train)\n",
    "params = CV.best_params_\n",
    "\n",
    "# 3 is an arbitrary choice, this can be changed \n",
    "\n",
    "loggmin = np.log2(params['gamma']) - 2\n",
    "loggmax = loggmin + 4\n",
    "\n",
    "logCmin = np.log2(params['C']) - 2\n",
    "logCmax = logCmin + 4\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888888888889 0.0691661088777\n",
      "[ 0.94444444  0.88888889  0.97222222  0.97222222  0.88888889  0.86111111\n",
      "  0.94444444  0.80555556  0.75        0.86111111]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=i)\n",
    "\n",
    "    \n",
    "\n",
    "    # Scaling the data to improve performance\n",
    "    scale = preprocessing.StandardScaler().fit(x_train)\n",
    "    x_train = scale.transform(x_train)\n",
    "    x_test = scale.transform(x_test)\n",
    "\n",
    "\n",
    "    # Parameters to search over\n",
    "    # Could search over different kernels, but I'm not convinced\n",
    "    # that will improve performance. It seems like gaussian kernels\n",
    "    # are as good as we'll get for problems like this\n",
    "    \n",
    "    # The range here is chosen from a coarse grid search (over 10-15 decades) that I did over \n",
    "    # a single segmentation. My reasoning was that the random segmentations wouldn't change the \n",
    "    # behavior over so many decades, but would matter for a finer grid search\n",
    "    # after finding the coarse best parameters. \n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': np.logspace(-8, -5, num = 10, base=2), 'C': np.logspace(0, 4, num = 10, base=2)}]\n",
    "\n",
    "    # Grid search over the parameters\n",
    "    # cv is another free parameter that I tuned manually\n",
    "    # This is another bottleneck: sometimes you can run into an issue\n",
    "    # where there are no positive instances in a cross-validation\n",
    "    # segmentation. More data will definitely improve this, but may also change\n",
    "    # the best-fit for this value.\n",
    "    CV = GridSearchCV(SVC(C=1), tuned_parameters, cv=10, scoring='accuracy')\n",
    "    CV.fit(x_train, y_train)\n",
    "\n",
    "    params = CV.best_params_\n",
    "    #print(params)\n",
    "\n",
    "    C = SVC(C = params['C'], gamma = params['gamma'])\n",
    "    C.fit(x_train, y_train)\n",
    "\n",
    "    #err, rec, prec, f1 = xi_alpha_est(C, x_train, y_train)\n",
    "\n",
    "    preds = C.predict(x_test)\n",
    "\n",
    "    \n",
    "\n",
    "    mets[i] = metrics.accuracy_score(y_test, preds)\n",
    "    #print(metrics.recall_score(y_test, preds), rec)\n",
    "    #print(metrics.precision_score(y_test, preds), prec)\n",
    "    #print(metrics.f1_score(y_test, preds), f1)\n",
    "    #print(y_train.size, y_test.size)\n",
    "    \n",
    "print(mets.mean(), mets.std())\n",
    "print(mets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
